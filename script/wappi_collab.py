# -*- coding: utf-8 -*-
"""Wappi отправка сообщений.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M6lSblCTJjXhQJFuRIQF2N4QQBy3hJq9
"""
import datetime

# fixes a bug with asyncio and jupyter
# import nest_asyncio
# nest_asyncio.apply()
import xmltodict
from langchain.document_loaders.sitemap import SitemapLoader
import requests
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import tiktoken
import matplotlib.pyplot as plt
import getpass
import os
import openai
from langchain.vectorstores import FAISS
import re


from langchain.document_loaders import AsyncHtmlLoader
from langchain.document_transformers import Html2TextTransformer
from langchain.chat_models import ChatOpenAI
from langchain.chains import create_extraction_chain

# Указываем конкретные странички для парсинга
urls = ["https://msk.laserlove.ru/", "https://laserlove.ru/franchise/blog/prokachka_project", "https://laserlove.ru/franchise/blog/medlicenziya"]
#urls = ["https://www.numizmatik.ru/shopcoins/delivery.php", "https://www.numizmatik.ru/payments.php", "https://www.numizmatik.ru/aboutdiscont"]


loader = AsyncHtmlLoader(urls)
docs = loader.load()

# трансформируем их в текст
html2text = Html2TextTransformer()
docs_transformed = html2text.transform_documents(docs)
# выводим первые 500 символов результата
docs_transformed[0].page_content[0:500]

# Получение ключа API от пользователя и установка его как переменной окружения

os.environ["OPENAI_API_KEY"] = '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'
openai.api_key = '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'

# инициализируем модель
llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-16k")

#  применяем create_extraction_chain для извлечения названия статьи
schema = {
    "properties": {
        "article_title": {"type": "string"}
    },
    "required": ["article_title"]
}

def extract(content: str, schema: dict):
    return create_extraction_chain(schema=schema, llm=llm).run(content)

# Применяем функцию extract ко всем документам
extracted_data_all = []

for doc in docs_transformed:
    content = doc.page_content

    # Применяем функцию extract
    extracted_content = extract(content=content, schema=schema)

    # Добавляем извлеченные данные в список (если нужно)
    extracted_data_all.append(extracted_content)



# полезно посмотреть на распределение длин чанков, чтобы понять, нужно ли их еще делить
def num_tokens_from_string(string: str, encoding_name: str) -> int:
      """Возвращает количество токенов в строке"""
      encoding = tiktoken.get_encoding(encoding_name)
      num_tokens = len(encoding.encode(string))
      return num_tokens
# Подсчет токенов для каждого фрагмента и построение графика
fragment_token_counts = [num_tokens_from_string(fragment.page_content, "cl100k_base") for fragment in docs_transformed]


def split_into_chunks(content: str, title: str, max_tokens: int = 1000, encoding_name: str = "cl100k_base") -> list:
    chunks = []
    start_index = 0

    while start_index < len(content):
        # Пытаемся взять максимум токенов
        end_index = start_index + max_tokens
        if end_index > len(content):
            end_index = len(content)

        # Находим последний символ \n\n или \n, чтобы разделить по нему
        split_index_nn = content.rfind("\n\n", start_index, end_index)
        split_index_n = content.rfind("\n", start_index, end_index)

        # Выбираем максимальный индекс из двух возможных вариантов разделения
        split_index = max(split_index_nn, split_index_n)

        # Если не можем найти \n\n или \n, используем end_index
        if split_index == -1:
            split_index = end_index

        chunk = content[start_index:split_index]

        # Проверяем, не превышает ли количество токенов 1000
        while num_tokens_from_string(f"# {title}\n\n{chunk}", encoding_name) > max_tokens and split_index > start_index:
            split_index_nn = content.rfind("\n\n", start_index, split_index-1)
            split_index_n = content.rfind("\n", start_index, split_index-1)

            # Аналогично выбираем максимальный индекс для нового разделения
            split_index = max(split_index_nn, split_index_n)
            chunk = content[start_index:split_index]

        # Добавляем чанк в список с добавлением заголовка и устанавливаем новый start_index
        chunks.append(f"# {title}\n\n{chunk}".strip())
        start_index = split_index + 1

    return chunks

# Применяем функцию для разбивки на чанки
chunks_all = []

for doc, extracted in zip(docs_transformed, extracted_data_all):
    title = extracted[0].get("article_title", "") if extracted else ""
    chunks = split_into_chunks(doc.page_content, title)
    chunks_all.extend(chunks)


# посмотрим на распределение длин чанков после разделения на более мелкие чанки
def num_tokens_from_strings(strings: list[str], encoding_name: str) -> list[int]:
    """Возвращает количество токенов в списке строк"""
    encoding = tiktoken.get_encoding(encoding_name)

    # Подсчет токенов для каждой строки и создание списка с количеством токенов
    num_tokens_per_string = [len(encoding.encode(string)) for string in strings]
    return num_tokens_per_string

# Если chunks_all - это просто список строк, используем его напрямую
fragment_token_counts = num_tokens_from_strings(chunks_all, "cl100k_base")

# Визуализируем результат


# Инициализирум модель эмбеддингов
embeddings = OpenAIEmbeddings()

# Создадим индексную базу из разделенных фрагментов текста
db_2 = FAISS.from_texts(chunks_all, embeddings)

system="Очень подробно и детально ответь на вопрос пользователя, опираясь точно на документ с информацией для ответа клиенту. Не придумывай ничего от себя. Не ссылайся на сами отрывки документ с информацией для ответа, клиент о них ничего не должен знать. Ответ дай не более 5 предложений. Не упоминай в ответе дату, имя, услугу на которую можно записаться."

def answer_index(system, topic, search_index, temp=0, verbose=0) -> str:
    """
    Функция возвращает ответ модели на основе заданной темы.
    """
    # находим наиболее релевантные вопросу пользователя чанки:
    docs = search_index.similarity_search(topic, k=6)
    message_content = re.sub(r'\n{2}', ' ', '\n '.join([f'\nОтрывок документа №{i+1}\n=====================' + doc.page_content + '\n' for i, doc in enumerate(docs)]))

    # если параметр verbose=1, то выводим релевантные чанки
    if verbose:
        print('message_content :\n', message_content)

    messages = [
        {"role": "system", "content": system},
  {"role": "user", "content": f"Ответь на вопрос пользователя, но не упоминай данные тебе документы с информацией в "
                              f"ответе. Документ с информацией для ответа пользователю: {message_content}\n\nВопрос "
                              f"пользователя: \n{topic}."}
    ]

    completion = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=temp
    )

    return completion.choices[0].message.content

